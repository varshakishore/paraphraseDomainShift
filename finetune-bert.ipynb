{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.5.0 available.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc811240390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from utils import loadData\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "MAX_LEN = 256\n",
    "NUM_LABELS = 2\n",
    "epochs = 25\n",
    "evalModel = True\n",
    "batch_size = 16\n",
    "save_path = '/home/vk352/paraphraseDomainShift/savedModels/bert_finetune_12000_1/'\n",
    "data_path = '/home/vk352/paraphraseDomainShift/data/'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# task options: quora, msr, twitter, paws\n",
    "task = 'not_paws_qqp'\n",
    "tasks=('msr', 'quora', 'twitter', 'paws', 'paws_qqp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, tokenizer):\n",
    "    # Given two sentences, x[\"string1\"] and x[\"string2\"], this function returns BERT ready inputs.\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            x[\"utt1\"],\n",
    "            x[\"utt2\"],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "            )\n",
    "\n",
    "    # First `input_ids` is a sequence of id-type representation of input string.\n",
    "    # Second `token_type_ids` is sequence identifier to show model the span of \"string1\" and \"string2\" individually.\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # BERT requires sequences in the same batch to have same length, so let's pad!\n",
    "    padding_length = MAX_LEN - len(input_ids)\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([pad_id] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([pad_id] * padding_length)\n",
    "\n",
    "    # Super simple validation.\n",
    "    assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
    "    assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
    "    assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
    "\n",
    "    # Convert them into PyTorch format.\n",
    "    label = torch.tensor(int(x[\"paraphrase\"])).long()\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "    # DONE!\n",
    "    return {\n",
    "            \"label\": label,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloaders(data_path, task, evalModel=False, tasks1=('msr', 'quora', 'twitter', 'paws', 'paws_qqp')):\n",
    "    \n",
    "    if task not in tasks:\n",
    "        train_l = []\n",
    "        test_l = []\n",
    "        val_l = []\n",
    "        for task in tasks1:\n",
    "            train, test, val = loadData(data_path, task)\n",
    "            train_l.append(train)\n",
    "            test_l.append(test)\n",
    "            val_l.append(val)\n",
    "        train = pd.concat(train_l)\n",
    "        test = pd.concat(test_l)\n",
    "        val = pd.concat(val_l)\n",
    "        \n",
    "        train = train.reset_index()\n",
    "        test = test.reset_index()\n",
    "        val = val.reset_index()\n",
    "            \n",
    "    else:\n",
    "        train, test, val = loadData(data_path, task)\n",
    "    \n",
    "    if evalModel:\n",
    "        test_data = test.apply(preprocess, axis=1, args=[tokenizer])\n",
    "        \n",
    "        test_dataloader = DataLoader(\n",
    "            list(test_data),\n",
    "            sampler=SequentialSampler(list(test_data)),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "        return test_dataloader\n",
    "\n",
    "\n",
    "    train_data = train.apply(preprocess, axis=1, args=[tokenizer])\n",
    "    val_data = val.apply(preprocess, axis=1, args=[tokenizer])\n",
    "    test_data = test.apply(preprocess, axis=1, args=[tokenizer])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_data,\n",
    "                sampler=RandomSampler(list(train_data)),\n",
    "                batch_size=batch_size\n",
    "                )\n",
    "    val_dataloader = DataLoader(\n",
    "                val_data,\n",
    "                sampler=SequentialSampler(list(val_data)),\n",
    "                batch_size=batch_size\n",
    "                )\n",
    "    test_dataloader = DataLoader(\n",
    "            list(test_data),\n",
    "            sampler=SequentialSampler(list(test_data)),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and set up dataloaders\n",
    "train_dataloader, test_dataloader, val_dataloader = getDataloaders(data_path, task, tasks1=('msr', 'quora', 'twitter', 'paws'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39264"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/vk352/transformers/examples/language-modeling/output/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/vk352/transformers/examples/language-modeling/output/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # pre-train BERT model\n",
    "# path = \"/home/vk352/transformers/examples/language-modeling/output/\"\n",
    "# model = BertForSequenceClassification.from_pretrained(path, num_labels=NUM_LABELS, return_dict=True)\n",
    "\n",
    "# # freeze all weights except classification layer\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 25 ========\n",
      "Training...\n",
      "0 completed epochs, 0 batches\n",
      "0 completed epochs, 1000 batches\n",
      "0 completed epochs, 2000 batches\n",
      "Training accuracy: 76.28\n",
      "Training loss: 0.45\n",
      "Validation...\n",
      " Val Accuracy: 83.80\n",
      "\n",
      "======== Epoch 2 / 25 ========\n",
      "Training...\n",
      "1 completed epochs, 0 batches\n",
      "1 completed epochs, 1000 batches\n",
      "1 completed epochs, 2000 batches\n",
      "Training accuracy: 89.17\n",
      "Training loss: 0.26\n",
      "Validation...\n",
      " Val Accuracy: 86.15\n",
      "\n",
      "======== Epoch 3 / 25 ========\n",
      "Training...\n",
      "2 completed epochs, 0 batches\n",
      "2 completed epochs, 1000 batches\n",
      "2 completed epochs, 2000 batches\n",
      "Training accuracy: 94.76\n",
      "Training loss: 0.14\n",
      "Validation...\n",
      " Val Accuracy: 86.12\n",
      "\n",
      "======== Epoch 4 / 25 ========\n",
      "Training...\n",
      "3 completed epochs, 0 batches\n",
      "3 completed epochs, 1000 batches\n",
      "3 completed epochs, 2000 batches\n",
      "Training accuracy: 97.10\n",
      "Training loss: 0.09\n",
      "Validation...\n",
      " Val Accuracy: 86.30\n",
      "\n",
      "======== Epoch 5 / 25 ========\n",
      "Training...\n",
      "4 completed epochs, 0 batches\n",
      "4 completed epochs, 1000 batches\n",
      "4 completed epochs, 2000 batches\n",
      "Training accuracy: 97.81\n",
      "Training loss: 0.06\n",
      "Validation...\n",
      " Val Accuracy: 86.05\n",
      "\n",
      "======== Epoch 6 / 25 ========\n",
      "Training...\n",
      "5 completed epochs, 0 batches\n",
      "5 completed epochs, 1000 batches\n",
      "5 completed epochs, 2000 batches\n",
      "Training accuracy: 98.30\n",
      "Training loss: 0.05\n",
      "Validation...\n",
      " Val Accuracy: 85.86\n",
      "\n",
      "======== Epoch 7 / 25 ========\n",
      "Training...\n",
      "6 completed epochs, 0 batches\n",
      "6 completed epochs, 1000 batches\n",
      "6 completed epochs, 2000 batches\n",
      "Training accuracy: 98.39\n",
      "Training loss: 0.04\n",
      "Validation...\n",
      " Val Accuracy: 85.67\n",
      "\n",
      "======== Epoch 8 / 25 ========\n",
      "Training...\n",
      "7 completed epochs, 0 batches\n",
      "7 completed epochs, 1000 batches\n",
      "7 completed epochs, 2000 batches\n",
      "Training accuracy: 98.72\n",
      "Training loss: 0.04\n",
      "Validation...\n",
      " Val Accuracy: 86.22\n",
      "\n",
      "======== Epoch 9 / 25 ========\n",
      "Training...\n",
      "8 completed epochs, 0 batches\n",
      "8 completed epochs, 1000 batches\n",
      "8 completed epochs, 2000 batches\n",
      "Training accuracy: 98.79\n",
      "Training loss: 0.03\n",
      "Validation...\n",
      " Val Accuracy: 86.24\n",
      "\n",
      "======== Epoch 10 / 25 ========\n",
      "Training...\n",
      "9 completed epochs, 0 batches\n",
      "9 completed epochs, 1000 batches\n",
      "9 completed epochs, 2000 batches\n",
      "Training accuracy: 98.84\n",
      "Training loss: 0.03\n",
      "Validation...\n",
      " Val Accuracy: 85.80\n",
      "\n",
      "======== Epoch 11 / 25 ========\n",
      "Training...\n",
      "10 completed epochs, 0 batches\n",
      "10 completed epochs, 1000 batches\n",
      "10 completed epochs, 2000 batches\n",
      "Training accuracy: 98.92\n",
      "Training loss: 0.03\n",
      "Validation...\n",
      " Val Accuracy: 86.15\n",
      "\n",
      "======== Epoch 12 / 25 ========\n",
      "Training...\n",
      "11 completed epochs, 0 batches\n",
      "11 completed epochs, 1000 batches\n",
      "11 completed epochs, 2000 batches\n",
      "Training accuracy: 98.86\n",
      "Training loss: 0.03\n",
      "Validation...\n",
      " Val Accuracy: 86.19\n",
      "\n",
      "======== Epoch 13 / 25 ========\n",
      "Training...\n",
      "12 completed epochs, 0 batches\n",
      "12 completed epochs, 1000 batches\n",
      "12 completed epochs, 2000 batches\n",
      "Training accuracy: 99.10\n",
      "Training loss: 0.03\n",
      "Validation...\n",
      " Val Accuracy: 85.83\n",
      "\n",
      "======== Epoch 14 / 25 ========\n",
      "Training...\n",
      "13 completed epochs, 0 batches\n",
      "13 completed epochs, 1000 batches\n",
      "13 completed epochs, 2000 batches\n",
      "Training accuracy: 99.04\n",
      "Training loss: 0.02\n",
      "Validation...\n",
      " Val Accuracy: 85.26\n",
      "\n",
      "======== Epoch 15 / 25 ========\n",
      "Training...\n",
      "14 completed epochs, 0 batches\n",
      "14 completed epochs, 1000 batches\n",
      "14 completed epochs, 2000 batches\n",
      "Training accuracy: 99.16\n",
      "Training loss: 0.02\n",
      "Validation...\n",
      " Val Accuracy: 85.42\n",
      "\n",
      "======== Epoch 16 / 25 ========\n",
      "Training...\n",
      "15 completed epochs, 0 batches\n",
      "15 completed epochs, 1000 batches\n",
      "15 completed epochs, 2000 batches\n",
      "Training accuracy: 99.17\n",
      "Training loss: 0.02\n",
      "Validation...\n",
      " Val Accuracy: 85.39\n",
      "\n",
      "======== Epoch 17 / 25 ========\n",
      "Training...\n",
      "16 completed epochs, 0 batches\n",
      "16 completed epochs, 1000 batches\n",
      "16 completed epochs, 2000 batches\n",
      "Training accuracy: 99.03\n",
      "Training loss: 0.02\n",
      "Validation...\n",
      " Val Accuracy: 85.33\n",
      "\n",
      "======== Epoch 18 / 25 ========\n",
      "Training...\n",
      "17 completed epochs, 0 batches\n",
      "17 completed epochs, 1000 batches\n",
      "17 completed epochs, 2000 batches\n",
      "Training accuracy: 99.19\n",
      "Training loss: 0.02\n",
      "Validation...\n",
      " Val Accuracy: 86.14\n",
      "\n",
      "======== Epoch 19 / 25 ========\n",
      "Training...\n",
      "18 completed epochs, 0 batches\n",
      "18 completed epochs, 1000 batches\n",
      "18 completed epochs, 2000 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-926d82d2fdce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtotal_train_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUM_LABELS, return_dict=True)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "best_dev_acc = 0\n",
    "\n",
    "# setup optimizer\n",
    "no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step%1000==0:\n",
    "            print('%d completed epochs, %d batches' % (epoch_i, step))\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(input_ids, \n",
    "                             token_type_ids=token_type_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)[:2]\n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        total_train_accuracy += ((preds == labels).cpu().numpy().mean() * 100)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "\n",
    "    print(\"Training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "    print(\"Training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    \n",
    "    print('Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss, logits = model(input_ids, \n",
    "                             token_type_ids=token_type_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)[:2]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        total_eval_accuracy += ((preds == labels).cpu().numpy().mean() * 100)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    print(\" Val Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    if avg_val_accuracy >= best_dev_acc:\n",
    "            torch.save(model.state_dict(), save_path+'bert_'+task+'.pt')\n",
    "            best_dev_acc = avg_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.2969483568075"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/vk352/transformers/examples/language-modeling/output/ were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/vk352/transformers/examples/language-modeling/output/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # pre-train BERT model\n",
    "# path = \"/home/vk352/transformers/examples/language-modeling/output/\"\n",
    "# model = BertForSequenceClassification.from_pretrained(path, num_labels=NUM_LABELS, return_dict=True)\n",
    "\n",
    "# # freeze all weights except classification layer\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n",
      "eval task:  paws_qqp\n",
      "not_paws_qqp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 62.22\n",
      "--------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "for task in tasks:\n",
    "    test_dataloader = getDataloaders(data_path, task, evalModel=evalModel)\n",
    "    print(len(test_dataloader)*16)\n",
    "    print(\"eval task: \", task)\n",
    "    for t in ['not_paws_qqp']:\n",
    "        print(t)\n",
    "        if evalModel:\n",
    "            model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUM_LABELS, return_dict=True)\n",
    "            model.load_state_dict(torch.load(save_path+'bert_'+t+'.pt'))\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            # test\n",
    "            model.eval()\n",
    "            total_test_accuracy = 0\n",
    "            total_test_loss = 0\n",
    "            for batch in test_dataloader:\n",
    "\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "\n",
    "                with torch.no_grad():        \n",
    "\n",
    "                    loss, logits = model(input_ids, \n",
    "                                     token_type_ids=token_type_ids, \n",
    "                                     attention_mask=attention_mask, \n",
    "                                     labels=labels)[:2]\n",
    "\n",
    "                # Accumulate the validation loss.\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(logits, dim=1).flatten()\n",
    "                total_test_accuracy += ((preds == labels).cpu().numpy().mean() * 100)\n",
    "\n",
    "\n",
    "            # Report the final accuracy for this run.\n",
    "            avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "            avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "            print(\" Test Accuracy: {0:.2f}\".format(avg_test_accuracy))\n",
    "    print('--------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
