{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from utils import loadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "MAX_LEN = 256\n",
    "NUM_LABELS = 2\n",
    "epochs = 50\n",
    "evalModel = False\n",
    "batch_size = 16\n",
    "save_path = '/home/vk352/paraphraseDomainShift/savedModels/'\n",
    "data_path = '/home/vk352/paraphraseDomainShift/data/'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# task options: quora, msr, twitter\n",
    "task = 'msr'\n",
    "train_task = 'msr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, tokenizer):\n",
    "    # Given two sentences, x[\"string1\"] and x[\"string2\"], this function returns BERT ready inputs.\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            x[\"utt1\"],\n",
    "            x[\"utt2\"],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            truncation=True\n",
    "            )\n",
    "\n",
    "    # First `input_ids` is a sequence of id-type representation of input string.\n",
    "    # Second `token_type_ids` is sequence identifier to show model the span of \"string1\" and \"string2\" individually.\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # BERT requires sequences in the same batch to have same length, so let's pad!\n",
    "    padding_length = MAX_LEN - len(input_ids)\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([pad_id] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([pad_id] * padding_length)\n",
    "\n",
    "    # Super simple validation.\n",
    "    assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
    "    assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
    "    assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
    "\n",
    "    # Convert them into PyTorch format.\n",
    "    label = torch.tensor(int(x[\"paraphrase\"])).long()\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "    # DONE!\n",
    "    return {\n",
    "            \"label\": label,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloaders(data_path, task, evalModel=False):\n",
    "    \n",
    "    train, test, val = loadData(data_path, task)\n",
    "    \n",
    "    if evalModel:\n",
    "        test_data = test.apply(preprocess, axis=1, args=[tokenizer])\n",
    "        \n",
    "        test_dataloader = DataLoader(\n",
    "            list(test_data),\n",
    "            sampler=SequentialSampler(list(test_data)),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "        return test_dataloader\n",
    "\n",
    "\n",
    "    train_data = train.apply(preprocess, axis=1, args=[tokenizer])\n",
    "    val_data = val.apply(preprocess, axis=1, args=[tokenizer])\n",
    "    test_data = test.apply(preprocess, axis=1, args=[tokenizer])\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "                train_data,\n",
    "                sampler=RandomSampler(list(train_data)),\n",
    "                batch_size=batch_size\n",
    "                )\n",
    "    val_dataloader = DataLoader(\n",
    "                val_data,\n",
    "                sampler=SequentialSampler(list(val_data)),\n",
    "                batch_size=batch_size\n",
    "                )\n",
    "    test_dataloader = DataLoader(\n",
    "            list(test_data),\n",
    "            sampler=SequentialSampler(list(test_data)),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and set up dataloaders\n",
    "train_dataloader, test_dataloader, val_dataloader = getDataloaders(data_path, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 50 ========\n",
      "Training...\n",
      "cuda:0\n",
      "0 completed epochs, 0 batches\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b28375cf5204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m                              \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                              labels=labels)[:2]\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_train_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUM_LABELS, return_dict=True)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "best_dev_acc = 0\n",
    "\n",
    "# setup optimizer\n",
    "no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step%1000==0:\n",
    "            print('%d completed epochs, %d batches' % (epoch_i, step))\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # the loss (because we provided labels) and the \"logits\"--the model\n",
    "        # outputs prior to activation.\n",
    "        loss, logits = model(input_ids, \n",
    "                             token_type_ids=token_type_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)[:2]\n",
    "        total_train_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        total_train_accuracy += ((preds == labels).cpu().numpy().mean() * 100)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "\n",
    "    print(\"Training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "    print(\"Training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    \n",
    "    print('Validation...')\n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss, logits = model(input_ids, \n",
    "                             token_type_ids=token_type_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)[:2]\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        total_eval_accuracy += ((preds == labels).cpu().numpy().mean() * 100)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    print(\" Val Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    if avg_val_accuracy >= best_dev_acc:\n",
    "            torch.save(model.state_dict(), save_path+'bert_'+task+'.pt')\n",
    "            best_dev_acc = avg_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 83.20\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "if evalModel:\n",
    "    test_dataloader = getDataloaders(data_path, task, evalModel=evalModel)\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUM_LABELS, return_dict=True)\n",
    "    model.load_state_dict(torch.load(save_path+'bert_'+train_task+'.pt'))\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # test\n",
    "    model.eval()\n",
    "    total_test_accuracy = 0\n",
    "    total_test_loss = 0\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss, logits = model(input_ids, \n",
    "                             token_type_ids=token_type_ids, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=labels)[:2]\n",
    "\n",
    "        # Accumulate the validation loss.\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        total_test_accuracy += ((preds == labels).cpu().numpy().mean() * 100)\n",
    "\n",
    "\n",
    "    # Report the final accuracy for this run.\n",
    "    avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    print(\" Test Accuracy: {0:.2f}\".format(avg_test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
