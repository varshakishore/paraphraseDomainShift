{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import pickle\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(text):\n",
    "    text = text.lower()\n",
    "    return ' '.join(word_tokenize(text)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/vk352/paraphraseDomainShift/data/'\n",
    "spm_path = '/home/vk352/paraphraseDomainShift/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quora data\n",
    "data_quora = pd.read_csv(data_path+'quora_duplicate_questions.tsv', sep='\\t')\n",
    "data_quora = data_quora[['question1', 'question2', 'is_duplicate']]\n",
    "data_quora.rename(columns={'is_duplicate': 'paraphrase'}, inplace=True)\n",
    "data_quora.dropna(inplace=True)\n",
    "data_quora['question1'] = data_quora['question1'].apply(lambda text: sanitize(text))\n",
    "data_quora['question2'] = data_quora['question2'].apply(lambda text: sanitize(text))\n",
    "# data_quora.to_pickle('data_quora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quora.to_pickle(data_path+'data_quora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/train/a.toks', train['question1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/train/a.toks', train['question2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/train/a.toks', train['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/test/a.toks', test['question1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/test/b.toks', test['question2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/test/sim.txt', test['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/dev/a.toks', dev['question1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/dev/b.toks', dev['question2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/dev/sim.txt', dev['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quora = pd.read_pickle(data_path+'data_quora')\n",
    "test = data_quora[-10000:]\n",
    "dev = data_quora[:-10000][-10000:]\n",
    "train = data_quora[:-20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msr_data \n",
    "with open(data_path+'msr-paraphrase-corpus/msr_paraphrase_train.txt', 'r') as file:\n",
    "    lines = file.readlines() \n",
    "data_msr = pd.DataFrame(data={'utt1': [line.split('\\t')[3] for line in lines[1:]],\n",
    "                             'utt2': [line.split('\\t')[4] for line in lines[1:]],\n",
    "                             'paraphrase': [line.split('\\t')[0] for line in lines[1:]]})\n",
    "with open(data_path+'msr-paraphrase-corpus/msr_paraphrase_test.txt', 'r') as file:\n",
    "    lines = file.readlines() \n",
    "data_msr_test = pd.DataFrame(data={'utt1': [line.split('\\t')[3] for line in lines[1:]],\n",
    "                             'utt2': [line.split('\\t')[4] for line in lines[1:]],\n",
    "                             'paraphrase': [line.split('\\t')[0] for line in lines[1:]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_msr['utt1'] = data_msr['utt1'].apply(lambda text: sanitize(text))\n",
    "data_msr['utt2'] = data_msr['utt2'].apply(lambda text: sanitize(text))\n",
    "data_msr_test['utt1'] = data_msr_test['utt1'].apply(lambda text: sanitize(text))\n",
    "data_msr_test['utt2'] = data_msr_test['utt2'].apply(lambda text: sanitize(text))\n",
    "split = int(0.8*len(data_msr))\n",
    "data_msr_train = data_msr[:split]\n",
    "data_msr_val = data_msr[split:]\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/train/a.toks', data_msr_train['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/train/b.toks', data_msr_train['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/train/sim.txt', data_msr_train['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/test/a.toks', data_msr_test['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/test/b.toks', data_msr_test['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/test/sim.txt', data_msr_test['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/dev/a.toks', data_msr_val['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/dev/b.toks', data_msr_val['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/dev/sim.txt', data_msr_val['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "from operator import itemgetter\n",
    "for sent in list(data_msr['utt1'])+list(data_msr['utt2']):\n",
    "    sent = sent.lower()\n",
    "    li.extend(word_tokenize(sent)[:-1])\n",
    "li = set(li)\n",
    "d = {}\n",
    "for i, elem in enumerate(li):\n",
    "    d[elem] = i\n",
    "# di = collections.OrderedDict(sorted(d.items(), key=itemgetter(1)))\n",
    "# pickle.dump( di, open( \"msr_vocab_cased.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\n",
    "sorted_dict = collections.OrderedDict(sorted(d.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13008, 13008)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_dict), len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter data\n",
    "data_twitter = pd.read_csv(data_path+'twitter/Twitter_URL_Corpus_train.txt', sep='\\t', header=None)\n",
    "data_twitter.columns = ['utt1', 'utt2', 'paraphrase', 'link']\n",
    "data_twitter = data_twitter[['utt1', 'utt2', 'paraphrase']]\n",
    "data_twitter['paraphrase'] = data_twitter['paraphrase'].apply(lambda x: int(re.search('(.),', x).group(1)))\n",
    "# original paper removes entries where annotators are split\n",
    "data_twitter = data_twitter[data_twitter['paraphrase']!=3] \n",
    "data_twitter['paraphrase'] = data_twitter['paraphrase'].apply(lambda x: '1' if x>=4 else '0')\n",
    "\n",
    "data_twitter_test = pd.read_csv(data_path+'twitter/Twitter_URL_Corpus_test.txt', sep='\\t', header=None)\n",
    "data_twitter_test.columns = ['utt1', 'utt2', 'paraphrase', 'link']\n",
    "data_twitter_test = data_twitter_test[['utt1', 'utt2', 'paraphrase']]\n",
    "data_twitter_test['paraphrase'] = data_twitter_test['paraphrase'].apply(lambda x: int(re.search('(.),', x).group(1)))\n",
    "# original paper removes entries where annotators are split\n",
    "data_twitter_test = data_twitter_test[data_twitter_test['paraphrase']!=3] \n",
    "data_twitter_test['paraphrase'] = data_twitter_test['paraphrase'].apply(lambda x: '1' if x>=4 else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter['utt1'] = data_twitter['utt1'].apply(lambda text: sanitize(text))\n",
    "data_twitter['utt2'] = data_twitter['utt2'].apply(lambda text: sanitize(text))\n",
    "data_twitter_test['utt1'] = data_twitter_test['utt1'].apply(lambda text: sanitize(text))\n",
    "data_twitter_test['utt2'] = data_twitter_test['utt2'].apply(lambda text: sanitize(text))\n",
    "split = int(0.8*len(data_twitter))\n",
    "data_twitter_train = data_twitter[:split]\n",
    "data_twitter_val = data_twitter[split:]\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/train/a.toks', data_twitter_train['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/train/b.toks', data_twitter_train['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/train/sim.txt', data_twitter_train['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/test/a.toks', data_twitter_test['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/test/b.toks', data_twitter_test['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/test/sim.txt', data_twitter_test['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/dev/a.toks', data_twitter_val['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/dev/b.toks', data_twitter_val['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/dev/sim.txt', data_twitter_val['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paws qqp\n",
    "data_paws_qqp = pd.read_csv(data_path+'PAWS/paws_qqp/train.tsv', sep='\\t')\n",
    "data_paws_qqp_dev_test = pd.read_csv(data_path+'PAWS/paws_qqp/dev_and_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paws\n",
    "data_paws_train = pd.read_csv(data_path+'PAWS/final/train.tsv', sep='\\t')\n",
    "data_paws_test = pd.read_csv(data_path+'PAWS/final/test.tsv', sep='\\t')\n",
    "data_paws_dev = pd.read_csv(data_path+'PAWS/final/dev.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49401, 8000, 8000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_paws_train), len(data_paws_dev), len(data_paws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
