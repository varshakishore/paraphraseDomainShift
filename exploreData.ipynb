{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import pickle\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(text):\n",
    "    text = text.decode('utf-8')\n",
    "    text = text.lower()\n",
    "    return ' '.join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/vk352/paraphraseDomainShift/data/'\n",
    "spm_path = '/home/vk352/paraphraseDomainShift/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-2eb13069b8c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_quora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "data_quora = pd.read_csv(data_path+'quora_duplicate_questions.tsv', sep='\\t')\n",
    "f = open(\"pretrainQuora.txt\", \"a\")\n",
    "\n",
    "for index, row in data_quora[12000:120000].iterrows():\n",
    "    f.write(row['question1']+'\\n')\n",
    "    f.write(row['question2']+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quora data\n",
    "data_quora = pd.read_csv(data_path+'quora_duplicate_questions.tsv', sep='\\t')\n",
    "data_quora = data_quora[['question1', 'question2', 'is_duplicate']]\n",
    "data_quora.rename(columns={'is_duplicate': 'paraphrase'}, inplace=True)\n",
    "data_quora.dropna(inplace=True)\n",
    "# data_quora['question1'] = data_quora['question1'].apply(lambda text: sanitize(text))\n",
    "# data_quora['question2'] = data_quora['question2'].apply(lambda text: sanitize(text))\n",
    "# data_quora.to_pickle('data_quora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quora.rename(columns={'is_duplicate': 'paraphrase', 'question1': 'utt1', 'question2': 'utt2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quora.to_pickle(data_path+'data_quora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/train/a.toks', train['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/train/b.toks', train['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/train/sim.txt', train['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/test/a.toks', test['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/test/b.toks', test['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/test/sim.txt', test['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/dev/a.toks', dev['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/dev/b.toks', dev['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/quora/dev/sim.txt', dev['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quora = pd.read_pickle(data_path+'data_quora')\n",
    "test = data_quora[-10000:]\n",
    "dev = data_quora[:-10000][-10000:]\n",
    "train = data_quora[:-20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msr_data \n",
    "with open(data_path+'msr-paraphrase-corpus/msr_paraphrase_train.txt', 'r') as file:\n",
    "    lines = file.readlines() \n",
    "data_msr = pd.DataFrame(data={'utt1': [line.split('\\t')[3] for line in lines[1:]],\n",
    "                             'utt2': [line.split('\\t')[4] for line in lines[1:]],\n",
    "                             'paraphrase': [line.split('\\t')[0] for line in lines[1:]]})\n",
    "with open(data_path+'msr-paraphrase-corpus/msr_paraphrase_test.txt', 'r') as file:\n",
    "    lines = file.readlines() \n",
    "data_msr_test = pd.DataFrame(data={'utt1': [line.split('\\t')[3] for line in lines[1:]],\n",
    "                             'utt2': [line.split('\\t')[4] for line in lines[1:]],\n",
    "                             'paraphrase': [line.split('\\t')[0] for line in lines[1:]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_msr['utt1'] = data_msr['utt1'].apply(lambda text: sanitize(text))\n",
    "data_msr['utt2'] = data_msr['utt2'].apply(lambda text: sanitize(text))\n",
    "data_msr_test['utt1'] = data_msr_test['utt1'].apply(lambda text: sanitize(text))\n",
    "data_msr_test['utt2'] = data_msr_test['utt2'].apply(lambda text: sanitize(text))\n",
    "split = int(0.8*len(data_msr))\n",
    "data_msr_train = data_msr[:split]\n",
    "data_msr_val = data_msr[split:]\n",
    "\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/train/a.toks', data_msr_train['utt1'].values, fmt='%s')\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/train/b.toks', data_msr_train['utt2'].values, fmt='%s')\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/train/sim.txt', data_msr_train['paraphrase'].values, fmt='%s')\n",
    "\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/test/a.toks', data_msr_test['utt1'].values, fmt='%s')\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/test/b.toks', data_msr_test['utt2'].values, fmt='%s')\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/test/sim.txt', data_msr_test['paraphrase'].values, fmt='%s')\n",
    "\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/dev/a.toks', data_msr_val['utt1'].values, fmt='%s')\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/dev/b.toks', data_msr_val['utt2'].values, fmt='%s')\n",
    "# np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/msr/dev/sim.txt', data_msr_val['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113435\n",
      "14800\n",
      "9288\n",
      "31344\n",
      "13342\n",
      "31511\n",
      "9072\n",
      "9164\n",
      "18178\n",
      "2038\n",
      "1974\n"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "li_set = set()\n",
    "for df in [data_quora, data_msr, data_msr_test, data_twitter, data_twitter_test, data_paws_train, data_paws_test, data_paws_dev,\n",
    "           data_paws_qqp_train, data_paws_qqp_test, data_paws_qqp_val]:\n",
    "    li = []\n",
    "    for sent in list(df['utt1'])+list(df['utt2']):\n",
    "        stext = sanitize(sent)\n",
    "        li.extend(stext.split(' '))\n",
    "    a = set(li)\n",
    "    li_set.update(a)\n",
    "    print(len(a))\n",
    "\n",
    "\n",
    "d = {}\n",
    "for i, elem in enumerate(li_set):\n",
    "    d[elem] = i\n",
    "# di = collections.OrderedDict(sorted(d.items(), key=itemgetter(1)))\n",
    "# pickle.dump( di, open( \"msr_vocab_cased.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( d, open( \"vocab.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pickle.load( open( \"/home/vk352/paraphraseDomainShift/SPM_toolkit/DecAtt/data/vocab.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136420"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(a.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\n",
    "sorted_dict = collections.OrderedDict(sorted(d.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13008, 13008)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_dict), len(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter data\n",
    "data_twitter = pd.read_csv(data_path+'twitter/Twitter_URL_Corpus_train.txt', sep='\\t', header=None)\n",
    "data_twitter.columns = ['utt1', 'utt2', 'paraphrase', 'link']\n",
    "data_twitter = data_twitter[['utt1', 'utt2', 'paraphrase']]\n",
    "data_twitter['paraphrase'] = data_twitter['paraphrase'].apply(lambda x: int(re.search('(.),', x).group(1)))\n",
    "# original paper removes entries where annotators are split\n",
    "data_twitter = data_twitter[data_twitter['paraphrase']!=3] \n",
    "data_twitter['paraphrase'] = data_twitter['paraphrase'].apply(lambda x: '1' if x>=4 else '0')\n",
    "\n",
    "data_twitter_test = pd.read_csv(data_path+'twitter/Twitter_URL_Corpus_test.txt', sep='\\t', header=None)\n",
    "data_twitter_test.columns = ['utt1', 'utt2', 'paraphrase', 'link']\n",
    "data_twitter_test = data_twitter_test[['utt1', 'utt2', 'paraphrase']]\n",
    "data_twitter_test['paraphrase'] = data_twitter_test['paraphrase'].apply(lambda x: int(re.search('(.),', x).group(1)))\n",
    "# original paper removes entries where annotators are split\n",
    "data_twitter_test = data_twitter_test[data_twitter_test['paraphrase']!=3] \n",
    "data_twitter_test['paraphrase'] = data_twitter_test['paraphrase'].apply(lambda x: '1' if x>=4 else '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter['utt1'] = data_twitter['utt1'].apply(lambda text: sanitize(text))\n",
    "data_twitter['utt2'] = data_twitter['utt2'].apply(lambda text: sanitize(text))\n",
    "data_twitter_test['utt1'] = data_twitter_test['utt1'].apply(lambda text: sanitize(text))\n",
    "data_twitter_test['utt2'] = data_twitter_test['utt2'].apply(lambda text: sanitize(text))\n",
    "split = int(0.8*len(data_twitter))\n",
    "data_twitter_train = data_twitter[:split]\n",
    "data_twitter_val = data_twitter[split:]\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/train/a.toks', data_twitter_train['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/train/b.toks', data_twitter_train['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/train/sim.txt', data_twitter_train['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/test/a.toks', data_twitter_test['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/test/b.toks', data_twitter_test['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/test/sim.txt', data_twitter_test['paraphrase'].values, fmt='%s')\n",
    "\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/dev/a.toks', data_twitter_val['utt1'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/dev/b.toks', data_twitter_val['utt2'].values, fmt='%s')\n",
    "np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/twitter/dev/sim.txt', data_twitter_val['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paws qqp\n",
    "data_paws_qqp_train = pd.read_csv(data_path+'PAWS/paws_qqp/train.tsv', sep='\\t', skiprows=1, names=['id', 'utt1', 'utt2', 'paraphrase'], encoding=\"utf-8\")\n",
    "data_paws_qqp_dev_test = pd.read_csv(data_path+'PAWS/paws_qqp/dev_and_test.tsv', sep='\\t', skiprows=1, names=['id', 'utt1', 'utt2', 'paraphrase'])\n",
    "\n",
    "# remove b''\n",
    "data_paws_qqp_train['utt1'] = data_paws_qqp_train.apply(lambda row: row['utt1'][2:-1],axis=1)\n",
    "data_paws_qqp_train['utt2'] = data_paws_qqp_train.apply(lambda row: row['utt2'][2:-1],axis=1)\n",
    "data_paws_qqp_dev_test['utt1'] = data_paws_qqp_dev_test.apply(lambda row: row['utt1'][2:-1],axis=1)\n",
    "data_paws_qqp_dev_test['utt2'] = data_paws_qqp_dev_test.apply(lambda row: row['utt2'][2:-1],axis=1)\n",
    "\n",
    "data_paws_qqp_val = data_paws_qqp_dev_test[:int(len(data_paws_qqp_dev_test)/2)]\n",
    "data_paws_qqp_test = data_paws_qqp_dev_test[int(len(data_paws_qqp_dev_test)/2):]\n",
    "data_paws_qqp_val = data_paws_qqp_val.reset_index()\n",
    "data_paws_qqp_test = data_paws_qqp_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name in zip([data_paws_qqp_train, data_paws_qqp_test, data_paws_qqp_val], ['train', 'test', 'dev']):\n",
    "    df['utt1'] = df['utt1'].apply(lambda text: sanitize(text))\n",
    "    df['utt2'] = df['utt2'].apply(lambda text: sanitize(text))\n",
    "    \n",
    "    np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/paws_qqp/%s/a.toks'%format(name), df['utt1'].values, fmt='%s')\n",
    "    np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/paws_qqp/%s/b.toks'%format(name), df['utt2'].values, fmt='%s')\n",
    "    np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/paws_qqp/%s/sim.txt'%format(name), df['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paws\n",
    "data_paws_train = pd.read_csv(data_path+'PAWS/final/train.tsv', sep='\\t', skiprows=1, names=['id', 'utt1', 'utt2', 'paraphrase'])\n",
    "data_paws_test = pd.read_csv(data_path+'PAWS/final/test.tsv', sep='\\t', skiprows=1, names=['id', 'utt1', 'utt2', 'paraphrase'])\n",
    "data_paws_dev = pd.read_csv(data_path+'PAWS/final/dev.tsv', sep='\\t', skiprows=1, names=['id', 'utt1', 'utt2', 'paraphrase'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name in zip([data_paws_train, data_paws_test, data_paws_dev], ['train', 'test', 'dev']):\n",
    "    df['utt1'] = df['utt1'].apply(lambda text: sanitize(text))\n",
    "    df['utt2'] = df['utt2'].apply(lambda text: sanitize(text))\n",
    "    \n",
    "    np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/paws/%s/a.toks'%format(name), df['utt1'].values, fmt='%s')\n",
    "    np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/paws/%s/b.toks'%format(name), df['utt2'].values, fmt='%s')\n",
    "    np.savetxt(spm_path+'SPM_toolkit/DecAtt/data/paws/%s/sim.txt'%format(name), df['paraphrase'].values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49401, 8000, 8000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_paws_train), len(data_paws_dev), len(data_paws_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = pickle.load( open( \"/home/vk352/paraphraseDomainShift/SPM_toolkit/DecAtt/data/vocab.pkl\", \"rb\" ) )\n",
    "tokens = list(tokens.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'mass.' in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'which one is correct ? \\u201c please let me know if you have any question \\u201d or \\u201d please let me know if you have any questions \\u201d'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = []\n",
    "s = 'which one is correct? “please let me know if you have any question” or ”please let me know if you have any questions”'\n",
    "sanitize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'in july , the hopkinton , mass. , company agreed to buy legato systems of mountain view for $ 1.3 billion .'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_msr['utt2'].apply(lambda text: sanitize(text)).iloc[2224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In July, the Hopkinton, Mass., company agreed to buy Legato Systems of Mountain View for $1.3 billion.\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_msr['utt2'].iloc[2224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
